## 7.1 形態素解析の実装(Janome, MeCab + NEologd)

- 機械学習による自然言語処理の流れを理解する
  1. **クリーニング** : 文章データ以外のテキスト(HTMLタグ，メールのヘッダ)の削除
  1. **正規化** : 表記(半角・全角，大文字・小文字)の統一，名寄せ，無意味な数字の置換
  1. **単語分割(形態素解析)** : 文章を複数の単語へ分割
  1. **基本形への変換(見出し語化)** : 各単語を基本形へ変換(例：走っ→走る)[省略可]
  1. **ストップワード除去** : 意味のない単語の除去(助詞，出現回数の多い単語など)[省略可]
  1. **単語の数値化** : ベクトル化 (ID化, 分散表現)
- JanomeおよびMeCab+NEologdを用いた形態素解析を実装できるようになる
  - Janome : pipからインストールして，形態素解析
  - MeCab : 新語辞書(NEologd)と組み合わせて，形態素解析．辞書を自分でカスタマイズできるため，新語に対応できる．

## 7.2 torchtextを用いたDataset, DataLoaderの実装

- torchtextを用いたDataset及びDataLoaderの実装ができる
  - 前処理と単語分割の関数を実装
  - torchtextを用いた(文章，ラベル)表の読み込み
    - torchtext.data.Fieldを用いて各列の読み込み方法を定義
    - torchtext.data.TabularDataset.splitsを用いて表の読み込みと単語分割
  - 単語のID化
    - 文章の単語分割結果を集計して，単語表(ボキャブラリ)を作成する
    - ボキャブラリは，登場回数(freq)とid(stoi)を含む
  - DataLoaderの作成
    - torchtext.data.Iteratorを用いる
      - 訓練用：train=True
      - 検証用：train=False, sort=False

## 7.3 単語の分散表現の仕組み(word2vec, fastText)

- 分散表現とは？
  - 機械学習で単語を扱うためには，単語のベクトル化が必要．
  - ベクトル化には，1-hotベクトルと分散表現の二つがある．
    - 1-hotベクトル
      - 長さ : ボキャブラリ中の単語数
      - 値 : 各単語のID番目の要素のみ1とし，他要素を0
    - 分散表現
      - 長さ : 任意の長さ(ボキャブラリ中の単語数よりはるかに小さい，数100次元)
      - 値 : 単語の意味を表す値
  - 分散表現のメリット
    - 長さが短いため，計算効率が良い
    - 単語の意味をベクトル演算で表現できる
      - 例：姫-女性+男性=王子
- word2vecで単語の分散表現を学習する仕組みを理解する
  - 概要
    - 大量の文章を解析し，各単語の意味を表す特徴量を，window幅数の周辺の単語との関係性から学習する
  - CBOW (従来手法)
    - 周辺の単語から中心の単語を予測できると考える
      - 例：貴族　の　たしなみ　を　学ぶ　**王子**　は　勇ましく　狩り　に　出かけ　まし　た
        - window幅1 
          - 学ぶ, は　→　王子
    - ニューラルネットワークで学習する
      - 構成：1-hot(周辺単語) -(W_in)→ 300次元 -(W_out)→ 1-hot(中心単語)
      - W_inを，各単語の分散表現とする
    - 補足：ネガティブサンプリング，階層化ソフトマックス (TODO)
  - Skip-gram (word2vecの手法)
    - 中心の単語から周辺の単語を予測できると考える
      - 例：貴族　の　たしなみ　を　学ぶ　**王子**　は　勇ましく　狩り　に　出かけ　まし　た
        - window幅1 
          - 王子　→　学ぶ，は
    - ニューラルネットワークで学習する
      - 構成：1-hot(中心単語) -(W_in)→ 300次元 -(W_out)→ 1-hot(周辺単語)
      - W_inを，各単語の分散表現とする
  - なぜSkip-gramの方が優れているか
    - 周辺の単語を予測する方が難しい
    - Skip-gramでは，活用時と同じく，学習時の入力が一語であるため
- fastTextで単語の分散表現を学習する仕組みを理解する
  - 概要
    - 未知語に対応するため，各単語の意味をサブワードの意味の重ね合わせととらえ，Skip-gramで学習させる
      - 例：when = (wh, whe, hen, en) + (whe, when, hen)
  - 学習高速化のための工夫(TODO)

## 7.4 word2vec, fastTextで日本語学習済みモデルを使用する方法

- 学習済みの日本語word2vecモデルで単語を分散表現に変換する実装ができるようになる
  - 公開モデルの読み込みと，torchtext用形式での保存(gensimを使用)
  - torchtextでの読み込みと，ボキャブラリ構築
    - build_vocabにて，vectorsに読み込んだ分散表現を指定する  
- 学習済みの日本語fastTextモデルで単語を分散表現に変換する実装ができるようになる
  - 同上．torchtextのデフォルトモデルは精度が低いとのこと．

## 7.5 IMDb(Internet Movie Database)のDataLoaderを実装

- テキスト形式のファイルデータからtsvファイルを作成し，torchtext用のDataLoaderを作成できるようになる
  - 7.6での学習向けに，各文章の冒頭を\<head>ではなく，\<cls>とする (init_tokenで指定)

## 7.6 Transformerの実装(分類タスク用)

- LSTMやRNNを使用せずCNNベースのTransformerで自然言語処理が可能な理由を理解する
  - 目的：文章→ラベル　モデルを訓練したい
    - 画像と文章の違い
      - 画像：サイズ(ピクセル数)固定，色を(r,g,b)でベクトル化できる
      - 文章：サイズ(単語数)可変，単語を分散表現でベクトル化できる
  - 従来：離れた単語間の関係性を活用して訓練するため，CNNではなく，RNNやLSTMを活用していた
    - CNNを活用する場合は固定幅入力とする必要がある．
      - 幅を大きくするとパラメータ数が多いため学習しきれない．
      - 幅を小さくすると，離れた位置にある単語間の関係性を活用できない．
    - 単語数が変わっても，文章内の単語間の関係性を活かして学習できるようにするため，以前に入力された単語情報を維持できるRNNやLSTMを活用していた
  - 課題：学習時間が長い
    - 1文章を学習するために，数10単語の入力が必要
  - 解決策：離れた位置にある単語間の類似度の高さ(Attention)を活用して，CNNで学習
    - 単語間の類似度が高さをAttentionとし，Attention Mapを活用して学習させる
- Transformer(エンコーダ)のモジュール構成を理解する，実装できるようになる
  - 感情分析でのネットワーク構成
    - 単語ID列 (bs x max_length)
    - Enbedder (max_length x dim_embedding; 分散表現化)
      - 実装方法
        - nn.Embeddingに，fastText等のベクトルを指定する
    - PositionalEncoder (max_length x dim_embedding; 位置情報テンソルの加算)
      - 実装方法
        - 位置情報テンソル算出式で算出したテンソルの加算 (TODO)
    - TransformerBlock x N (max_length x dim_embedding; 特徴量変換)
      - 補足
        - 任意回繰り返す
        - maskを使用：無視したいAttentionを0にする(例：単語数 < max_length時の\<pad\>)
      - ネットワーク構成
        - 第一段階
          - LayerNormalization + Attention(multi-headed attention) + Dropout
        - 第二段階
          - LayerNormalization + FeedForward(全結合層x2) + Dropout
    - ClassificationHead (dim_output; 全結合層を使って出力ラベルの予測)
      - 補足
        - 文章全体を要約した特徴量を使うために，TransformerBlockの出力のうち，冒頭トークン\<cls>に対応した特徴量のみを活用する．
          - 学習によって，\<cls>の特徴量として，文章全体を要約した特徴量を獲得できる．

## 7.7 Transformerの学習・推論，判定根拠の可視化を実装

- Transformerの学習を実装できるようになる
  - TransformerBlockの初期化：活性化関数ReLuのため，Heの初期値を使用
- Transformerの判定時のAttention可視化を実装できるようになる
  - Attentionを結果を可視化するメリット
    - 判断根拠が分かる
    - モデルの改善方針を検討できる
